{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "### Benign and malignant cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "dataset = pd.read_csv(\"../data/Cancer_Data.csv\")\n",
    "\n",
    "if 'Unnamed: 32' in dataset.columns:\n",
    "    dataset.drop('Unnamed: 32', axis=1, inplace=True)\n",
    "dataset['diagnosis'].replace(['B', 'M'],[0, 1], inplace=True) # B = 0, M = 1 \n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = dataset.drop(['diagnosis','id'],axis=1)\n",
    "y = dataset['diagnosis']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will analyse the dataset with all information, and for that, we first need to create a decision tree.\n",
    "Our first decision tree will be a default one, created by the scikit learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier() \n",
    "clf.get_params()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we send the decision tree our train sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the predictions we call the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(x_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision_score(y_test, predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this code, we can see the importance given to each feature of the dataset by the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_worst = pd.DataFrame(clf.feature_importances_, index = x_test.columns).sort_values(0, ascending=False)\n",
    "feature_importance_worst.columns = ['%']\n",
    "feature_importance_worst"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset, we have 3 type of data:\n",
    " - worst: worst value recorded from that person\n",
    " - se: standard error\n",
    " - mean: mean a list of values\n",
    "\n",
    "To see if we can improve our accuracy and precision, we are going to try to separate these 3 types and we will run it with the same decision tree to see if our results improve.<br>\n",
    "We will also use the same lines as x_train and x_test so we can compare our results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset only with \"Worst\" values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_worst_train = x_train.filter(['fractal_dimension_worst','symmetry_worst','concave points_worst','concavity_worst','compactness_worst','smoothness_worst','area_worst','perimeter_worst','texture_worst','radius_worst'])\n",
    "x_worst_test = x_test.filter(['fractal_dimension_worst','symmetry_worst','concave points_worst','concavity_worst','compactness_worst','smoothness_worst','area_worst','perimeter_worst','texture_worst','radius_worst'])\n",
    "\n",
    "\n",
    "clf.fit(x_worst_train,y_train)\n",
    "predictions_worst = clf.predict(x_worst_test)\n",
    "accuracy_score(y_test, predictions_worst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, predictions_worst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "recall_score(y_test,predictions_worst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_worst = pd.DataFrame(clf.feature_importances_, index = x_worst_test.columns).sort_values(0, ascending=False)\n",
    "feature_importance_worst.columns = ['%']\n",
    "feature_importance_worst"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset only with \"Mean\" values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean_train = x_train.filter(['fractal_dimension_mean','symmetry_mean','concave points_mean','concavity_mean','compactness_mean','smoothness_mean','area_mean','perimeter_mean','texture_mean','radius_mean'])\n",
    "x_mean_test = x_test.filter(['fractal_dimension_mean','symmetry_mean','concave points_mean','concavity_mean','compactness_mean','smoothness_mean','area_mean','perimeter_mean','texture_mean','radius_mean'])\n",
    "\n",
    "\n",
    "clf.fit(x_mean_train,y_train)\n",
    "predictions_mean = clf.predict(x_mean_test)\n",
    "accuracy_score(y_test, predictions_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, predictions_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_mean = pd.DataFrame(clf.feature_importances_, index = x_mean_test.columns).sort_values(0, ascending=False)\n",
    "feature_importance_mean.columns = ['%']\n",
    "feature_importance_mean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset only with \"SE\" values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_se_train = x_train.filter(['fractal_dimension_se','symmetry_se','concave points_se','concavity_se','compactness_se','smoothness_se','area_se','perimeter_se','texture_se','radius_se'])\n",
    "x_se_test = x_test.filter(['fractal_dimension_se','symmetry_se','concave points_se','concavity_se','compactness_se','smoothness_se','area_se','perimeter_se','texture_se','radius_se'])\n",
    "\n",
    "\n",
    "clf.fit(x_se_train,y_train)\n",
    "predictions_se = clf.predict(x_se_test)\n",
    "accuracy_score(y_test, predictions_se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, predictions_se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_se = pd.DataFrame(clf.feature_importances_, index = x_se_test.columns).sort_values(0, ascending=False)\n",
    "feature_importance_se.columns = ['%']\n",
    "feature_importance_se"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset without \"SE\" values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the SE values got the worst results, we will try to use the all the dataset but without the SE values to see if the predictions are better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_worstMean_train = x_train.drop(['fractal_dimension_se','symmetry_se','concave points_se','concavity_se','compactness_se','smoothness_se','area_se','perimeter_se','texture_se','radius_se'], axis=1)\n",
    "x_worstMean_test = x_test.drop(['fractal_dimension_se','symmetry_se','concave points_se','concavity_se','compactness_se','smoothness_se','area_se','perimeter_se','texture_se','radius_se'], axis=1)\n",
    "\n",
    "clf.fit(x_worstMean_train,y_train)\n",
    "predictions_worstMean = clf.predict(x_worstMean_test)\n",
    "accuracy_score(y_test, predictions_worstMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, predictions_worstMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_worstMean = pd.DataFrame(clf.feature_importances_, index = x_worstMean_test.columns).sort_values(0, ascending=False)\n",
    "feature_importance_worstMean.columns = ['%']\n",
    "feature_importance_worstMean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gathered 30 results from each dataset and calculated the mean of each one to see the differences.<br>\n",
    "<br>\n",
    "\n",
    "The average accuracy of each dataset was:\n",
    " - \"Worst\" Values: 0,93410\n",
    " - Without \"SE\" Values: 0,92577 \n",
    " - Normal Datase: 0,92027\n",
    " - \"Mean\" Values: 0,90863\n",
    " - \"SE\" Values: 0,87820\n",
    "\n",
    "<br>\n",
    "\n",
    "![Alt text](../images/decision_tree_accuracy.png)\n",
    "\n",
    "<br>\n",
    "In this graph we can see that the dataset with the \"worst\" columns has the best results in terms of accuracy but it's still very close to the dataset without the Standard Error (SE) columns and with the Normal dataset.\n",
    "<br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same happened with the precision, being the dataset with the \"Worst\" values the best one.\n",
    "\n",
    " - \"Worst\" Values: 0,91085\n",
    " - Without \"SE\" Values: 0,90221\n",
    "  - Normal Datase: 0,89370\n",
    " - \"Mean\" Values: 0,87820\n",
    " - \"SE\" Values: 0,76504\n",
    "\n",
    "<br>\n",
    "\n",
    "![Alt text](../images/decision_tree_precision.png)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that the values are all very close might be explained by the fact that the Decision tree algorithm uses the greedy algorithm at each split finding a local optima, since \"worst\" values give the best results, all the datasets containing that values are going to be very similar. <br>\n",
    "A way to improve decision trees is to avoid overfitting. This could be made by limiting maximum depth, pruning etc.\n",
    "When we prune a tree we are correcting it after it has been fitted to the training dataset. It starts at the leaf nodes and removes those branches that do not affect the overall tree accuracy. It also lowers the complexity of the model.\n",
    "<br>\n",
    "In our code we can use sklearn to prune our trees using the ``ccp_alpha`` attribute inside the ``DecisionTreeClassifier`` function.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset with \"worst\" values using pruning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prune a tree using sklearn we have to set the attribute `ccp_alpha` inside the `DecisionTreeClassifier` function but this value varies with each tree.<br>\n",
    "A way to calculate the best alpha value for each tree is to get the different applicable alpha values. Then we just run the same decision tree with the different alpha values to get the accuracy.<br>\n",
    "The code below shows the relation between alpha and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = clf.cost_complexity_pruning_path(x_worst_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "clfs = []\n",
    "\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf_worst = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "    clf_worst.fit(x_worst_train, y_train)\n",
    "    clfs.append(clf_worst)\n",
    "\n",
    "acc_scores = [accuracy_score(y_test, clf_worst.predict(x_worst_test)) for clf_worst in clfs]\n",
    "\n",
    "plt.figure(figsize=(10,  6))\n",
    "plt.grid()\n",
    "plt.plot(ccp_alphas[:-1], acc_scores[:-1])\n",
    "plt.xlabel(\"effective alpha\")\n",
    "plt.ylabel(\"Accuracy scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "accuracy_values = []\n",
    "\n",
    "for i in range(100):\n",
    "    x_alpha = x.filter(['fractal_dimension_worst','symmetry_worst','concave points_worst','concavity_worst','compactness_worst','smoothness_worst','area_worst','perimeter_worst','texture_worst','radius_worst'])\n",
    "\n",
    "    x_alpha_train, x_alpha_test, y_alpha_train, y_alpha_test = train_test_split(x_alpha, y, test_size=0.3)\n",
    "    \n",
    "    clf_alpha = DecisionTreeClassifier() \n",
    "\n",
    "    path = clf_alpha.cost_complexity_pruning_path(x_alpha_train, y_alpha_train)\n",
    "    ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "    clfs = []\n",
    "\n",
    "    for ccp_alpha in ccp_alphas:\n",
    "        clf_alpha = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "        clf_alpha.fit(x_alpha_train, y_alpha_train)\n",
    "        clfs.append(clf_alpha)\n",
    "\n",
    "    acc_scores = [accuracy_score(y_alpha_test, clf_alpha.predict(x_alpha_test)) for clf_alpha in clfs]\n",
    "    accuracy_values.append(max(acc_scores))\n",
    "\n",
    "mean(accuracy_values)\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
