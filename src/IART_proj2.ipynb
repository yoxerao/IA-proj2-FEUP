{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "### Benign and malignant cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n",
    "from statistics import mean\n",
    "\n",
    "dataset = pd.read_csv(\"../data/Cancer_Data.csv\")\n",
    "\n",
    "if 'Unnamed: 32' in dataset.columns:\n",
    "    dataset.drop('Unnamed: 32', axis=1, inplace=True)\n",
    "dataset['diagnosis'].replace(['B', 'M'],[0, 1], inplace=True) # B = 0, M = 1 \n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(dataset.corr(),cbar=True,annot=True,cmap='Oranges')\n",
    "print(dataset.corr())\n",
    "plt.show()\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = dataset.drop(['diagnosis','id'],axis=1)\n",
    "y = dataset['diagnosis']\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will analyse the dataset with all information, and for that, we first need to create a decision tree.\n",
    "Our first decision tree will be a default one, created by the scikit learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
    "clf = DecisionTreeClassifier() \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we send the decision tree our train sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the predictions we call the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = clf.predict(x_test)\n",
    "\n",
    "dt_accuracy = accuracy_score(y_test, predictions)\n",
    "dt_precision = precision_score(y_test, predictions)\n",
    "dt_recall = recall_score(y_test, predictions)\n",
    "dt_f1 = f1_score(y_test, predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " (We also defined a function to plot the confusion matrix, which we will use later on). MUDAR ISTO PARA FAZER SENTIDO, TIVE QUE BAZAR E YA UELELEEEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot(y_pred, accuracy, precision, recall, f1_score):\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.imshow(cm, cmap=plt.cm.Oranges)\n",
    "    plt.title(\"Confusion Matrix\\nAccuracy: {:.3f} - Precision: {:.3f} - Recall: {:.3f} - F1 Score: {:.3f}\".format(accuracy, precision, recall, f1_score))\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"Predicted Class\")\n",
    "    plt.ylabel(\"True Class\")\n",
    "    plt.xticks([0, 1], [\"Benign\", \"Malignant\"])\n",
    "    plt.yticks([0, 1], [\"Benign\", \"Malignant\"])\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            plt.text(j, i, str(cm[i, j]), ha='center', va='center', color='black')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plot(predictions,dt_accuracy,dt_precision,dt_recall,dt_f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this code, we can see the importance given to each feature of the dataset by the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_worst = pd.DataFrame(clf.feature_importances_, index = x_test.columns).sort_values(0, ascending=False)\n",
    "feature_importance_worst.columns = ['%']\n",
    "feature_importance_worst"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our dataset, we have 3 type of data:\n",
    " - worst: worst value recorded from that person\n",
    " - se: standard error\n",
    " - mean: mean a list of values\n",
    "\n",
    "To see if we can improve our accuracy and precision, we are going to try to separate these 3 types and we will run it with the same decision tree to see if our results improve.<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset only with \"Worst\" values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_worst_train = x_train.filter(['fractal_dimension_worst','symmetry_worst','concave points_worst','concavity_worst','compactness_worst','smoothness_worst','area_worst','perimeter_worst','texture_worst','radius_worst'])\n",
    "x_worst_test = x_test.filter(['fractal_dimension_worst','symmetry_worst','concave points_worst','concavity_worst','compactness_worst','smoothness_worst','area_worst','perimeter_worst','texture_worst','radius_worst'])\n",
    "\n",
    "\n",
    "clf.fit(x_worst_train,y_train)\n",
    "predictions_worst = clf.predict(x_worst_test)\n",
    "dt_worst_accuracy = accuracy_score(y_test, predictions_worst)\n",
    "dt_worst_precision = precision_score(y_test, predictions_worst)\n",
    "dt_worst_recall = recall_score(y_test, predictions_worst)\n",
    "dt_worst_f1 = f1_score(y_test, predictions_worst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plot(predictions_worst,dt_worst_accuracy,dt_worst_precision,dt_worst_recall,dt_worst_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_worst = pd.DataFrame(clf.feature_importances_, index = x_worst_test.columns).sort_values(0, ascending=False)\n",
    "feature_importance_worst.columns = ['%']\n",
    "feature_importance_worst"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset only with \"Mean\" values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean_train = x_train.filter(['fractal_dimension_mean','symmetry_mean','concave points_mean','concavity_mean','compactness_mean','smoothness_mean','area_mean','perimeter_mean','texture_mean','radius_mean'])\n",
    "x_mean_test = x_test.filter(['fractal_dimension_mean','symmetry_mean','concave points_mean','concavity_mean','compactness_mean','smoothness_mean','area_mean','perimeter_mean','texture_mean','radius_mean'])\n",
    "\n",
    "\n",
    "clf.fit(x_mean_train,y_train)\n",
    "predictions_mean = clf.predict(x_mean_test)\n",
    "dt_mean_accuracy = accuracy_score(y_test, predictions_mean)\n",
    "dt_mean_precision = precision_score(y_test, predictions_mean)\n",
    "dt_mean_recall = recall_score(y_test, predictions_mean)\n",
    "dt_mean_f1 = f1_score(y_test, predictions_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plot(predictions_mean,dt_mean_accuracy,dt_mean_precision,dt_mean_recall,dt_mean_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_mean = pd.DataFrame(clf.feature_importances_, index = x_mean_test.columns).sort_values(0, ascending=False)\n",
    "feature_importance_mean.columns = ['%']\n",
    "feature_importance_mean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset only with \"SE\" values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_se_train = x_train.filter(['fractal_dimension_se','symmetry_se','concave points_se','concavity_se','compactness_se','smoothness_se','area_se','perimeter_se','texture_se','radius_se'])\n",
    "x_se_test = x_test.filter(['fractal_dimension_se','symmetry_se','concave points_se','concavity_se','compactness_se','smoothness_se','area_se','perimeter_se','texture_se','radius_se'])\n",
    "\n",
    "\n",
    "clf.fit(x_se_train,y_train)\n",
    "predictions_se = clf.predict(x_se_test)\n",
    "dt_se_accuracy = accuracy_score(y_test, predictions_se)\n",
    "dt_se_precision = precision_score(y_test, predictions_se)\n",
    "dt_se_recall = recall_score(y_test, predictions_se)\n",
    "dt_se_f1 = f1_score(y_test, predictions_se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plot(predictions_se,dt_se_accuracy,dt_se_precision,dt_se_recall,dt_se_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_se = pd.DataFrame(clf.feature_importances_, index = x_se_test.columns).sort_values(0, ascending=False)\n",
    "feature_importance_se.columns = ['%']\n",
    "feature_importance_se"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To examine which dataset gives the best results, we will run each dataset 100 times and compare its results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_accuracy_mean = []\n",
    "dt_precision_mean = []\n",
    "dt_recall_mean = []\n",
    "dt_f1_mean = []\n",
    "\n",
    "dt_worst_accuracy_mean = []\n",
    "dt_worst_precision_mean = []\n",
    "dt_worst_recall_mean = []\n",
    "dt_worst_f1_mean = []\n",
    "\n",
    "dt_mean_accuracy_mean = []\n",
    "dt_mean_precision_mean = []\n",
    "dt_mean_recall_mean = []\n",
    "dt_mean_f1_mean = []\n",
    "\n",
    "dt_se_accuracy_mean = []\n",
    "dt_se_precision_mean = []\n",
    "dt_se_recall_mean = []\n",
    "dt_se_f1_mean = []\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(x_train,y_train)\n",
    "    predictions = clf.predict(x_test)\n",
    "\n",
    "    dt_accuracy_mean.append(accuracy_score(y_test, predictions))\n",
    "    dt_precision_mean.append(precision_score(y_test, predictions))\n",
    "    dt_recall_mean.append(recall_score(y_test, predictions))\n",
    "    dt_f1_mean.append(f1_score(y_test, predictions))\n",
    "\n",
    "\n",
    "    x_worst_train = x_train.filter(['fractal_dimension_worst','symmetry_worst','concave points_worst','concavity_worst','compactness_worst','smoothness_worst','area_worst','perimeter_worst','texture_worst','radius_worst'])\n",
    "    x_worst_test = x_test.filter(['fractal_dimension_worst','symmetry_worst','concave points_worst','concavity_worst','compactness_worst','smoothness_worst','area_worst','perimeter_worst','texture_worst','radius_worst'])\n",
    "\n",
    "    clf.fit(x_worst_train,y_train)\n",
    "    predictions_worst = clf.predict(x_worst_test)\n",
    "\n",
    "    dt_worst_accuracy_mean.append(accuracy_score(y_test, predictions_worst))\n",
    "    dt_worst_precision_mean.append(precision_score(y_test, predictions_worst))\n",
    "    dt_worst_recall_mean.append(recall_score(y_test, predictions_worst))\n",
    "    dt_worst_f1_mean.append(f1_score(y_test, predictions_worst))\n",
    "\n",
    "\n",
    "    x_mean_train = x_train.filter(['fractal_dimension_mean','symmetry_mean','concave points_mean','concavity_mean','compactness_mean','smoothness_mean','area_mean','perimeter_mean','texture_mean','radius_mean'])\n",
    "    x_mean_test = x_test.filter(['fractal_dimension_mean','symmetry_mean','concave points_mean','concavity_mean','compactness_mean','smoothness_mean','area_mean','perimeter_mean','texture_mean','radius_mean'])\n",
    "\n",
    "    clf.fit(x_mean_train,y_train)\n",
    "    predictions_mean = clf.predict(x_mean_test)\n",
    "\n",
    "    dt_mean_accuracy_mean.append(accuracy_score(y_test, predictions_mean))\n",
    "    dt_mean_precision_mean.append(precision_score(y_test, predictions_mean))\n",
    "    dt_mean_recall_mean.append(recall_score(y_test, predictions_mean))\n",
    "    dt_mean_f1_mean.append(f1_score(y_test, predictions_mean))\n",
    "\n",
    "\n",
    "    x_se_train = x_train.filter(['fractal_dimension_se','symmetry_se','concave points_se','concavity_se','compactness_se','smoothness_se','area_se','perimeter_se','texture_se','radius_se'])\n",
    "    x_se_test = x_test.filter(['fractal_dimension_se','symmetry_se','concave points_se','concavity_se','compactness_se','smoothness_se','area_se','perimeter_se','texture_se','radius_se'])\n",
    "\n",
    "    clf.fit(x_se_train,y_train)\n",
    "    predictions_se = clf.predict(x_se_test)\n",
    "\n",
    "    dt_se_accuracy_mean.append(accuracy_score(y_test, predictions_se))\n",
    "    dt_se_precision_mean.append(precision_score(y_test, predictions_se))\n",
    "    dt_se_recall_mean.append(recall_score(y_test, predictions_se))\n",
    "    dt_se_f1_mean.append(f1_score(y_test, predictions_se))\n",
    "\n",
    "def show_mean_plot(type,normal,worst,mean_dt,se):\n",
    "    plt.title(\"Normal: {:.4f} - Worst: {:.4f} - Mean: {:.4f} - SE: {:.4f}\".format(mean(normal), mean(worst), mean(mean_dt), mean(se)))\n",
    "    plt.plot(normal, label = 'Normal ' + type)\n",
    "    plt.plot(worst, label = 'Worst ' + type)\n",
    "    plt.plot(mean_dt, label = 'Mean ' + type)\n",
    "    plt.plot(se, label = 'SE ' + type)\n",
    "    plt.legend(loc = 'lower right')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_mean_plot(\"accuracy\",dt_accuracy_mean,dt_worst_accuracy_mean,dt_mean_accuracy_mean,dt_se_accuracy_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_mean_plot(\"precision\",dt_precision_mean,dt_worst_precision_mean,dt_mean_precision_mean,dt_se_precision_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_mean_plot(\"recall\",dt_recall_mean,dt_worst_recall_mean,dt_mean_recall_mean,dt_se_recall_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_mean_plot(\"f1_score\",dt_f1_mean,dt_worst_f1_mean,dt_mean_f1_mean,dt_se_f1_mean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, in general, the worst dataset produces the best results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that the values are all very close might be explained by the fact that the Decision tree algorithm uses the greedy algorithm at each split finding a local optima, since \"worst\" values give the best results, all the datasets containing that values are going to be very similar. <br>\n",
    "A way to improve decision trees is to avoid overfitting. This could be made by limiting maximum depth, pruning etc.\n",
    "When we prune a tree we are correcting it after it has been fitted to the training dataset. It starts at the leaf nodes and removes those branches that do not affect the overall tree accuracy. It also lowers the complexity of the model.\n",
    "<br>\n",
    "In our code we can use sklearn to prune our trees using the ``ccp_alpha`` attribute inside the ``DecisionTreeClassifier`` function.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said before, our tree could be overfitted, to overcome that we will try to find a better model using Grid Search Cross Validation. As parameters, we will use the criterion and compare 'gini' and 'entropy', max_depth which we will compare 5 different values, min_samples_split with a range between 2 and 10,min_samples_lead with a range between 1 and 5, ccp_alpha to prune the tree with values from 0 to 0.03 with an interval of 0.005 to try and find the best ccp_alpha, and finally, max_features with 'sqrt' and 'log2'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
    "clf = DecisionTreeClassifier() \n",
    "x_worst_train = x_train.filter(['fractal_dimension_worst','symmetry_worst','concave points_worst','concavity_worst','compactness_worst','smoothness_worst','area_worst','perimeter_worst','texture_worst','radius_worst'])\n",
    "x_worst_test = x_test.filter(['fractal_dimension_worst','symmetry_worst','concave points_worst','concavity_worst','compactness_worst','smoothness_worst','area_worst','perimeter_worst','texture_worst','radius_worst'])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"criterion\": ['gini','entropy'],\n",
    "    \"max_depth\": [4,5,6,7,8],\n",
    "    \"min_samples_split\": range(2,10),\n",
    "    \"min_samples_leaf\": range(1,5),\n",
    "    \"ccp_alpha\": [0,0.005,0.01,0.015,0.02,0.025,0.03],\n",
    "    \"max_features\": ['sqrt','log2']\n",
    "}\n",
    "clf = DecisionTreeClassifier() \n",
    "cv = KFold(n_splits=10)\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, scoring='recall', cv=cv) # Perform grid search with cross-validation\n",
    "grid_search.fit(x_worst_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "best_prediction = best_model.predict(x_worst_test)\n",
    "\n",
    "best_accuracy = accuracy_score(y_test, best_prediction)\n",
    "best_precision = precision_score(y_test, best_prediction)\n",
    "best_recall = recall_score(y_test, best_prediction)\n",
    "best_f1 = f1_score(y_test, best_prediction)\n",
    "\n",
    "show_plot(best_prediction, best_accuracy, best_precision, best_recall, best_f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having our improved model, we can see its parameters using `.get_params()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.get_params()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good thing about decision tree, is that we can visualize the path followed by the algorithm to predict it's values. The next code, prints us the tree of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = tree.plot_tree(best_model, \n",
    "                   feature_names=x_worst_train.columns,  \n",
    "                   class_names={0:'Malignant', 1:'Benign'},\n",
    "                   filled=True,\n",
    "                  fontsize=12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppot vector machines (SVM) attempt to find a hyperplane in an space of N dimensions, where N is the number of classification attributes. <br>This hyperplane is used to separate the data into two distinct classes, in our case, benign and malignant cancer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same datasets as before, and start by an SVM classifier object and fit it to our training data. We then make a prediction using the test data and calculate the accuracy and precision, as well as the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_score, accuracy_score\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(x_train, y_train)\n",
    "\n",
    "svm.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm.predict(x_test)\n",
    "\n",
    "\n",
    "svm_accuracy = accuracy_score(y_test, y_pred)\n",
    "svm_precision = precision_score(y_test, y_pred)\n",
    "svm_recall = recall_score(y_test, y_pred)\n",
    "svm_f1 = f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_plot(y_pred, svm_accuracy, svm_precision, svm_recall, svm_f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the confusion matrix, the SVM classifier shows promising results. However, we can try to improve it by removing attributes that could muddle the model. This would also reduce the number of dimensions we have to deal with, which simplifies the model.\n",
    "<br> We'll do this by dividing the features into three groups: worst, mean and standard error. We'll then run the SVM classifier on each group and compare the results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset with only \"Worst\" features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_worst_train = x_train.filter(['fractal_dimension_worst','symmetry_worst','concave points_worst','concavity_worst','compactness_worst','smoothness_worst','area_worst','perimeter_worst','texture_worst','radius_worst'])\n",
    "x_worst_test = x_test.filter(['fractal_dimension_worst','symmetry_worst','concave points_worst','concavity_worst','compactness_worst','smoothness_worst','area_worst','perimeter_worst','texture_worst','radius_worst'])\n",
    "\n",
    "svm.fit(x_worst_train,y_train)\n",
    "svm_predictions_worst = svm.predict(x_worst_test)\n",
    "\n",
    "svm_accuracy_worst = accuracy_score(y_test, svm_predictions_worst)\n",
    "svm_precision_worst = precision_score(y_test, svm_predictions_worst)\n",
    "svm_recall_worst = recall_score(y_test, svm_predictions_worst)\n",
    "svm_f1_worst = f1_score(y_test, svm_predictions_worst)\n",
    "\n",
    "show_plot(svm_predictions_worst, svm_accuracy_worst, svm_precision_worst, svm_recall_worst, svm_f1_worst)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset with only \"Mean\" features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean_train = x_train.filter(['fractal_dimension_mean','symmetry_mean','concave points_mean','concavity_mean','compactness_mean','smoothness_mean','area_mean','perimeter_mean','texture_mean','radius_mean'])\n",
    "x_mean_test = x_test.filter(['fractal_dimension_mean','symmetry_mean','concave points_mean','concavity_mean','compactness_mean','smoothness_mean','area_mean','perimeter_mean','texture_mean','radius_mean'])\n",
    "\n",
    "\n",
    "\n",
    "svm.fit(x_mean_train,y_train)\n",
    "svm_predictions_mean = svm.predict(x_mean_test)\n",
    "\n",
    "svm_accuracy_mean = accuracy_score(y_test, svm_predictions_mean)\n",
    "svm_precision_mean = precision_score(y_test, svm_predictions_mean)\n",
    "svm_recall_mean = recall_score(y_test, svm_predictions_mean)\n",
    "svm_f1_mean = f1_score(y_test, svm_predictions_mean)\n",
    "\n",
    "show_plot(svm_predictions_mean, svm_accuracy_mean, svm_precision_mean, svm_recall_mean, svm_f1_mean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset with only \"Standard Error\" features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_se_train = x_train.filter(['fractal_dimension_se','symmetry_se','concave points_se','concavity_se','compactness_se','smoothness_se','area_se','perimeter_se','texture_se','radius_se'])\n",
    "x_se_test = x_test.filter(['fractal_dimension_se','symmetry_se','concave points_se','concavity_se','compactness_se','smoothness_se','area_se','perimeter_se','texture_se','radius_se'])\n",
    "\n",
    "\n",
    "\n",
    "svm.fit(x_se_train,y_train)\n",
    "svm_predictions_se = svm.predict(x_se_test)\n",
    "\n",
    "svm_accuracy_se = accuracy_score(y_test, svm_predictions_se)\n",
    "svm_precision_se = precision_score(y_test, svm_predictions_se)\n",
    "svm_recall_se = recall_score(y_test, svm_predictions_se)\n",
    "svm_f1_se = f1_score(y_test, svm_predictions_se)\n",
    "\n",
    "show_plot(svm_predictions_se, svm_accuracy_se, svm_precision_se, svm_recall_se, svm_f1_se)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous tests we can conclude that, just like with the decision tree, the dataset with the worst features has the best results.\n",
    "<br> We can also see that SVM gets better values all around compared to the decision tree, which is expected since SVM can handle outliers better than the decision tree by allowing some misclassification. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are getting good results but we can still try to improve our model. From now on we'll only use the dataset with the \"Worst\" features, and we'll try to improve the model by tuning the hyperparameters. We'll use the GridSearchCV function from sklearn to find the best hyperparameters for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 4], #penalty parameter of error term -> increase can lead to over fitting\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'degree': [2, 3, 4], #1 == linear, \n",
    "    'gamma': ['scale', 'auto'], #the higher the gamma, the more influence closer points have\n",
    "    'shrinking': [True, False],\n",
    "    'verbose': [2]\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=10)\n",
    "\n",
    "grid_search = GridSearchCV(svm, param_grid, scoring='recall', cv=cv) # Perform grid search with cross-validation\n",
    "grid_search.fit(x_worst_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "best_prediction = best_model.predict(x_worst_test)\n",
    "\n",
    "best_accuracy = accuracy_score(y_test, best_prediction)\n",
    "best_precision = precision_score(y_test, best_prediction)\n",
    "best_recall = recall_score(y_test, best_prediction)\n",
    "best_f1 = f1_score(y_test, best_prediction)\n",
    "\n",
    "show_plot(best_prediction, best_accuracy, best_precision, best_recall, best_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
